{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook guides through the training of a random forest machine learning regressor. The data is split into a training and a test set, by default with a relation of 9:1. The expression values are scaled to zero mean and unit variance based on the training set. Positions that are non-informative because no alternative nucleotides have been tested are deleted. The performance evaluation is based on the R^2 score from sklearn. The correlation of measured and predicted expression values is plotted. The feature importance from the random forest regression represent the contributions of each nucleotide-position to the prediction. They are extracted and visualized with a Logo-plot.\n",
    "\n",
    "If you generate a new regressor with new train-test set division, make sure you put the current date on the `ML_Date parameter` in your `config.txt`.\n",
    "\n",
    "If you generate a new regressor based on an existing train-test set division, use the date of the existing train-test set division on the `ML_Date parameter` in your `config.txt`. \n",
    "\n",
    "Make sure the `ML_Regressor` parameter in the `config.txt` file represents your favorit ML-approach and the parameter `Data_Standard` is set to `True` for SVR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System initiation\n",
    "\n",
    "Loading all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import time\n",
    "import timeit\n",
    "import joblib\n",
    "import pickle\n",
    "from ExpressionExpert_Functions import Data_Src_Load, make_DataDir, split_train_test, ExpressionScaler, Sequence_Conserved_Adjusted, MyRFR, MySVR, MyGBR\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable setting\n",
    "\n",
    "We load the naming conventions from 'config.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_Dict = dict()\n",
    "with open('config.txt') as Conf:\n",
    "    myline = Conf.read().splitlines()\n",
    "    for line in myline:\n",
    "        if not line.startswith('#'):\n",
    "            (key, val) = line.split(':', 1)\n",
    "            Name_Dict[str(key.strip())] = val.strip()\n",
    "        \n",
    "\n",
    "Data_File = Name_Dict['Data_File']\n",
    "# extract the filename for naming of newly generated files\n",
    "File_Base = Name_Dict['File_Base']\n",
    "# the generated files will be stored in a subfolder with custom name\n",
    "Data_Folder = Name_Dict['Data_Folder']\n",
    "# column name of expression values\n",
    "Y_Col_Name = eval(Name_Dict['Y_Col_Name'])\n",
    "# Extracting entropy cutoff for removal of non-informative positions\n",
    "Entropy_cutoff = float(Name_Dict['Entropy_cutoff'])\n",
    "# figure file type\n",
    "Fig_Type = Name_Dict['Figure_Type']\n",
    "# Figure font size\n",
    "FigFontSize = Name_Dict['Figure_Font_Size']\n",
    "make_DataDir(Name_Dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "General information on the data source csv-file is stored in the 'config.txt' file generated in the '0-Workflow' notebook. The sequence and expression data is stored in a csv file with an identifier in column 'ID' (not used for anything), the DNA-sequence in column 'Sequence', and the expression strength in column 'promoter activity'. While loading, the sequence is converted to a label encrypted sequence, ['A','C','G','T'] replaced by [0,1,2,3], and a one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeqDat = Data_Src_Load(Name_Dict)\n",
    "SeqDat.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation\n",
    "\n",
    "For the machine learning the data is first separated into training and test sets. The training set is used to generate a standard scaler for expression standardization to zero mean and unit variance. On each position the entropy is calculated to assess how much nucleotide diversity has been sampled on each position. If at any position the entropy is zero, i.e. only one nucleotide is present in all samples, this position is removed because it is non-informative for further analysis (Position entropy analysis). \n",
    "\n",
    "### Split data to train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can generate a new train-test split or use an existing file. \n",
    "# If the ML_Date is from the current date, it is assumed you generate a new regressor\n",
    "# If another ML_Date than the current date is chosen, an existing train-test division is loaded\n",
    "# The identifier is based on the date from the corresponding machine learning training.\n",
    "# True generates a new split, False loads an existing split\n",
    "ML_Date = Name_Dict['ML_Date']\n",
    "GenSplit = True if ML_Date == time.strftime('%Y%m%d') else False\n",
    "if GenSplit:\n",
    "    print('new train-test')\n",
    "    train_size = 1 - eval(Name_Dict['TestRatio'])\n",
    "    # split number '1' because we only use one final test set. Cross validation comes later\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=train_size)\n",
    "    X = SeqDat['Sequence']\n",
    "    y = SeqDat[Y_Col_Name]\n",
    "    groups = SeqDat['Sequence_letter-encrypted'].str.upper()\n",
    "    Train_Idx, Test_Idx = list(gss.split(X, y, groups))[0]\n",
    "    SeqTest = SeqDat.iloc[Test_Idx].reset_index(drop=True)\n",
    "    SeqTrain = SeqDat.iloc[Train_Idx].reset_index(drop=True)\n",
    "\n",
    "    TrainTest_Data = {'Train': SeqTrain, 'Test': SeqTest}\n",
    "    TrainTest_File = os.path.join(Data_Folder, '{}_{}_TrainTest-Data.pkl'.format(time.strftime('%Y%m%d'), File_Base))\n",
    "    pickle.dump(TrainTest_Data, open(TrainTest_File, 'wb'))\n",
    "\n",
    "    print('Train and test data stored as: {}'.format(TrainTest_File))\n",
    "else:\n",
    "    TrainTest_File = os.path.join(Data_Folder, '{}_{}_TrainTest-Data.pkl'.format(ML_Date, File_Base))\n",
    "    TrainTest_Data = pickle.load(open(TrainTest_File,'rb'))\n",
    "    SeqTrain, SeqTest = TrainTest_Data['Train'], TrainTest_Data['Test']\n",
    "    print('Load existing Train-Test split.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data engineering\n",
    "\n",
    "Normalization of the data improves training for kernel and artificial neural network based strategies. However, omit this step for correlation and regression tree (CART) approaches. \n",
    "\n",
    "If at any position the entropy is below a threshold (`Entropy_cutoff`) because too few nucleotides are sampled, this position is removed because it is non-informative for further analysis (Position entropy analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization step, omit for CART approaches by setting variable 'Data_Standard' to 'False'\n",
    "if eval(Name_Dict['Data_Standard']):\n",
    "    SeqTrain, Expr_Scaler = ExpressionScaler(SeqTrain, Name_Dict)\n",
    "\n",
    "# removing non-informative positions where no base diversity exists, base one hot encoding\n",
    "SeqTrain_Hadj, Positions_removed, PSEntropy = Sequence_Conserved_Adjusted(SeqTrain, Name_Dict, Entropy_cutoff=Entropy_cutoff)\n",
    "\n",
    "print('Normalization: {}'.format(Name_Dict['Data_Standard']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with grid search on shuffle split\n",
    "\n",
    "You can either choose to start a new training of a random forest regressor or load an existing regressor. If you load an existing random-forest regressor the parameters of the standard scaler are loaded based on names in the config-file. For the estimation the training set is dynamically separated into a new training and test set with a 9:1 ratio (parameter 'test_ratio') with 1000 random shuffle splits (parameter 'split_number'). The training takes about 5 minutes on 16 cpu-cores.\n",
    "\n",
    "**User input:** <br>\n",
    " * Decision whether a new random-forest training is started or an existing regressor is loaded.\n",
    " \n",
    "*Example:*<br>\n",
    " Start new random-forest training by setting:<br>\n",
    " RFR_File = 0<br>\n",
    " otherwise, insert the file adress:<br>\n",
    " RFR_File = 'data-Example1-Pput\\\\20191106_Example1-Pput_RFR_ML-File.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of independent promoter library measurements\n",
    "Measure_Numb = int(Name_Dict['Library_Expression'])\n",
    "ML_Type = Name_Dict['ML_Regressor']\n",
    "Regressor_Best = dict()\n",
    "# ML Random Forest training for number of independent promoter library measurements\n",
    "for Meas_Idx in range(Measure_Numb):\n",
    "    # starting the machine learning\n",
    "    # This can take a while\n",
    "    start_time = timeit.default_timer()\n",
    "    test_ratio = .1\n",
    "    split_number = 100\n",
    "    # if the data is standardized we have to extract it from the separate column, otherwise we use the original data column\n",
    "    Meas_Name = '{}_scaled'.format(Y_Col_Name[Meas_Idx]) if eval(Name_Dict['Data_Standard']) else Y_Col_Name[Meas_Idx]\n",
    "    AddFeat = eval(Name_Dict['Add_Feat'])\n",
    "    print('Starting new {} regressor training for {}'.format(ML_Type, Meas_Name))\n",
    "    # starting regression with regressor type defined in the config file\n",
    "    # RFR: random forest regression\n",
    "    if ML_Type == 'RFR':\n",
    "        MyReg = MyRFR(SeqTrain_Hadj, test_ratio, split_number, Meas_Name, AddFeat)\n",
    "    # SVR: support vector regression\n",
    "    elif ML_Type == 'SVR':\n",
    "        MyReg = MySVR(SeqTrain_Hadj, test_ratio, split_number, Meas_Name, AddFeat)\n",
    "    # TPOT: TPOT automated tree regression\n",
    "    elif ML_Type == 'GBR':\n",
    "        MyReg = MyGBR(SeqTrain_Hadj, test_ratio, split_number, Meas_Name, AddFeat)\n",
    "    elif ML_Type == 'TPOT':\n",
    "        print('TPOT')\n",
    "    else:\n",
    "        print('Regressor type not recognized, choose:')\n",
    "        print('\"RFR\": random forest regression')\n",
    "        print('\"SVR\": support vector regression')\n",
    "        print('\"TPOT\": automated tree regression pipeline (requires TPOT installation)')\n",
    "        break\n",
    "        \n",
    "    run_time = timeit.default_timer() - start_time\n",
    "    print('Training run time: {:.0f} sec'.format(run_time))\n",
    "\n",
    "    # getting the best estimator\n",
    "    Regressor_Best = MyReg.best_estimator_\n",
    "\n",
    "    # saving the best estimator\n",
    "    Regressor_File = os.path.join(Data_Folder, '{}_{}_{}_{}-Regressor.pkl'.format(time.strftime('%Y%m%d'), File_Base, Y_Col_Name[Meas_Idx].replace(' ','-'), ML_Type))\n",
    "    joblib.dump(Regressor_Best, Regressor_File)\n",
    "    # conserved positions not used as input for the regressor\n",
    "    Data_Prep_Params = {'Positions_removed': Positions_removed}\n",
    "    # Mean and standard deviation of training set expression used for normalizing\n",
    "    if eval(Name_Dict['Data_Standard']):\n",
    "        # The standard scaler default name is the name of the expression measurement column with suffix: '_Scaler'    \n",
    "        Scaler_DictName = '{}_Scaler'.format(Y_Col_Name[Meas_Idx])\n",
    "        Data_Prep_Params[Scaler_DictName] = Expr_Scaler[Scaler_DictName]\n",
    "\n",
    "    Parameter_File = os.path.join(Data_Folder, '{}_{}_{}_{}-Params.pkl'.format(time.strftime('%Y%m%d'), File_Base, Y_Col_Name[Meas_Idx].replace(' ','-'), ML_Type))\n",
    "    pickle.dump(Data_Prep_Params, open(Parameter_File, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
