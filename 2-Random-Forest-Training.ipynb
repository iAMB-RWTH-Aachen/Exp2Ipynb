{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest regressor training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook guides through the training of a random forest machine learning regressor. The data is split into a training and a test set, by default with a relation of 9:1. The expression values are scaled to zero mean and unit variance based on the training set. Positions that are non-informative because no alternative nucleotides have been tested are deleted. The performance evaluation is based on the R^2 score from sklearn. The correlation of measured and predicted expression values is plotted. The feature importance from the random forest regression represent the contributions of each nucleotide-position to the prediction. They are extracted and visualized with a Logo-plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System initiation\n",
    "\n",
    "Loading all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import time\n",
    "import timeit\n",
    "import joblib\n",
    "import pickle\n",
    "from ExpressionExpert_Functions import Data_Src_Load, make_DataDir, split_train_test, ExpressionScaler, Sequence_Conserved_Adjusted, Est_Grad_Save, Est_Grad_Feat\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable setting\n",
    "\n",
    "We load the naming conventions from 'config.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_Dict = dict()\n",
    "with open('config_EcolPtai.txt') as Conf:\n",
    "    myline = Conf.read().splitlines()\n",
    "    for line in myline:\n",
    "        if not line.startswith('#'):\n",
    "            (key, val) = line.split(':', 1)\n",
    "            Name_Dict[str(key.strip())] = val.strip()\n",
    "        \n",
    "\n",
    "Data_File = Name_Dict['Data_File']\n",
    "# extract the filename for naming of newly generated files\n",
    "File_Base = Name_Dict['File_Base']\n",
    "# the generated files will be stored in a subfolder with custom name\n",
    "Data_Folder = Name_Dict['Data_Folder']\n",
    "# column name of expression values\n",
    "Y_Col_Name = eval(Name_Dict['Y_Col_Name'])\n",
    "# figure file type\n",
    "Fig_Type = Name_Dict['Figure_Type']\n",
    "make_DataDir(Name_Dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "General information on the data source csv-file is stored in the 'config.txt' file generated in the '0-Workflow' notebook. The sequence and expression data is stored in a csv file with an identifier in column 'ID' (not used for anything), the DNA-sequence in column 'Sequence', and the expression strength in column 'promoter activity'. While loading, the sequence is converted to a label encrypted sequence, ['A','C','G','T'] replaced by [0,1,2,3], and a one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeqDat = Data_Src_Load(Name_Dict)\n",
    "SeqDat.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation\n",
    "\n",
    "For the machine learning the data is first separated into training and test sets. The training set is used to generate a standard scaler for expression standardization to zero mean and unit variance. On each position the entropy is calculated to assess how much nucleotide diversity has been sampled on each position. If at any position the entropy is zero, i.e. only one nucleotide is present in all samples, this position is removed because it is non-informative for further analysis (Position entropy analysis). \n",
    "\n",
    "### Split data to train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SeqTrain, SeqTest = split_train_test(SeqDat)\n",
    "train_size = 1 - eval(Name_Dict['TestRatio'])\n",
    "# split number '1' because we only use one final test set. Cross validation comes later\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=train_size)\n",
    "X = SeqDat['Sequence']\n",
    "y = SeqDat[Y_Col_Name]\n",
    "groups = SeqDat['Sequence_letter-encrypted'].str.upper()\n",
    "Train_Idx, Test_Idx = list(gss.split(X, y, groups))[0]\n",
    "SeqTest = SeqDat.iloc[Test_Idx].reset_index(drop=True)\n",
    "SeqTrain = SeqDat.iloc[Train_Idx].reset_index(drop=True)\n",
    "\n",
    "TrainTest_Data = {'Train': SeqTrain, 'Test': SeqTest}\n",
    "TrainTest_File = os.path.join(Data_Folder, '{}_{}_TrainTest-Data.pkl'.format(time.strftime('%Y%m%d'), File_Base))\n",
    "pickle.dump(TrainTest_Data, open(TrainTest_File, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeqTrain, Expr_Scaler = ExpressionScaler(SeqTrain, Name_Dict)\n",
    "# removing non-informative positions where no base diversity exists, base one hot encoding\n",
    "SeqTrain_Hadj, Positions_removed, PSEntropy = Sequence_Conserved_Adjusted(SeqTrain, Name_Dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest regression with grid search on shuffle split\n",
    "\n",
    "You can either choose to start a new training of a random forest regressor or load an existing regressor. If you load an existing random-forest regressor the parameters of the standard scaler are loaded based on names in the config-file. For the estimation the training set is dynamically separated into a new training and test set with a 9:1 ratio (parameter 'test_ratio') with 1000 random shuffle splits (parameter 'split_number'). The training takes about 5 minutes on 16 cpu-cores.\n",
    "\n",
    "**User input:** <br>\n",
    " * Decision whether a new random-forest training is started or an existing regressor is loaded.\n",
    " \n",
    "*Example:*<br>\n",
    " Start new random-forest training by setting:<br>\n",
    " RFR_File = 0<br>\n",
    " otherwise, insert the file adress:<br>\n",
    " RFR_File = 'data-Example1-Pput\\\\20191106_Example1-Pput_RFR_ML-File.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ExpressionExpert_Functions import my_SVR\n",
    "# Number of independent promoter library measurements\n",
    "Measure_Numb = int(Name_Dict['Library_Expression'])\n",
    "RFR_Best = dict()\n",
    "# ML Random Forest training for number of independent promoter library measurements\n",
    "for Meas_Idx in range(Measure_Numb):\n",
    "    print('Starting new regressor training')\n",
    "\n",
    "    # starting the machine learning with random forest and grid search\n",
    "    # This can take a while\n",
    "    start_time = timeit.default_timer()\n",
    "    test_ratio = .1\n",
    "    split_number = 100\n",
    "    Norm_Meas_Input = '{}_scaled'.format(Y_Col_Name[Meas_Idx])\n",
    "    AddFeat = eval(Name_Dict['Add_Feat'])\n",
    "    forest_regr = Est_Grad_Feat(SeqTrain_Hadj, test_ratio, split_number, Norm_Meas_Input, AddFeat)\n",
    "#     forest_regr = my_SVR(SeqTrain_Hadj, Norm_Meas_Input)\n",
    "    run_time = timeit.default_timer() - start_time\n",
    "    print('grid search on {} measurements, run time: {:.0f} sec'.format(Y_Col_Name[Meas_Idx], run_time))\n",
    "\n",
    "    # prediction of training and test data sets\n",
    "    # getting the best estimator\n",
    "#     RFR_Best = forest_regr\n",
    "    RFR_Best = forest_regr.best_estimator_\n",
    "\n",
    "    # saving the best estimator\n",
    "    ML_ID = Name_Dict['RFR_ML_File']\n",
    "    Regressor_File = os.path.join(Data_Folder, '{}_{}_{}_{}.pkl'.format(time.strftime('%Y%m%d'), File_Base, Y_Col_Name[Meas_Idx].replace(' ','-'), ML_ID))\n",
    "    joblib.dump(RFR_Best, Regressor_File)\n",
    "    # saving: \n",
    "    # 1. conserved positions not used as input for the regressor\n",
    "    # 2. Mean and standard deviation of training set expression used for normalizing\n",
    "    # 3. The standard scaler default name is the name of the expression measurement column with suffix: '_Scaler'\n",
    "    Scaler_DictName = '{}_Scaler'.format(Y_Col_Name[Meas_Idx])\n",
    "    Data_Prep_Params = {'Positions_removed': Positions_removed, Scaler_DictName: Expr_Scaler[Scaler_DictName]}\n",
    "    Param_ID = Name_Dict['RFR_Params_File']\n",
    "    Parameter_File = os.path.join(Data_Folder, '{}_{}_{}_{}.pkl'.format(time.strftime('%Y%m%d'), File_Base, Y_Col_Name[Meas_Idx].replace(' ','-'), Param_ID))\n",
    "    pickle.dump(Data_Prep_Params, open(Parameter_File, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeqTrain_Hadj['Sequence'].values.tolist()\n",
    "import joblib\n",
    "\n",
    "myRFR = joblib.load('data-XHost_Replicates_Seq+Spacer/20200413_Spacer_all_Ptai-Activity_RFR_ML-File.pkl')\n",
    "Nr_Feat = len(myRFR.feature_importances_)\n",
    "GC_Importance = Nr_Feat - np.arange(Nr_Feat)[np.argsort(myRFR.feature_importances_)==Nr_Feat-1]\n",
    "print('Position of GC-content in List: {}'.format(GC_Importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = np.reshape(np.array(SeqTrain_Hadj['OneHot'].values.tolist()), (len(SeqTrain_Hadj),-1))\n",
    "Y = SeqTrain_Hadj[Norm_Meas_Input].values.tolist()\n",
    "Number_Estimators = np.arange(20,35,1)\n",
    "Max_Features = np.arange(9,15,1)\n",
    "param_grid = [{'bootstrap':[False], 'n_estimators': Number_Estimators, 'max_features': Max_Features}]\n",
    "\n",
    "ML_sub = RandomForestRegressor()\n",
    "gridML = GridSearchCV(ML_sub, param_grid, cv=5, n_jobs=-1)\n",
    "gridML.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit, GroupShuffleSplit\n",
    "\n",
    "Norm_Meas_Input = '{}_scaled'.format(Y_Col_Name[0])\n",
    "# myML = RandomForestRegressor(gridML.best_params_)\n",
    "X = np.reshape(np.array(SeqTrain_Hadj['OneHot'].values.tolist()), (len(SeqTrain_Hadj),-1))\n",
    "Y = SeqTrain_Hadj[Norm_Meas_Input].values.tolist()\n",
    "groups = SeqTrain_Hadj['Sequence_letter-encrypted']\n",
    "splits = 20\n",
    "test_size = .1\n",
    "cv = GroupShuffleSplit(n_splits=splits, test_size=test_size) #, random_state=42\n",
    "scores = cross_val_score(gridML.best_estimator_, X, Y, cv=cv, groups=groups)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
