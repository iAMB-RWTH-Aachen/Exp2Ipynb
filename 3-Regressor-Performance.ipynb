{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest regressor analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook guides through the analysis of an existing random forest regressor. The performance evaluation is based on the R^2 score from sklearn. The correlation of measured and predicted expression values is plotted. The feature importance from the random forest regression represent the contributions of each nucleotide-position to the prediction. They are extracted and visualized with a Logo-plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System initiation\n",
    "\n",
    "Loading all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logomaker as lm\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import pickle\n",
    "from math import sqrt\n",
    "from ExpressionExpert_Functions import Data_Src_Load, split_train_test, list_onehot, Insert_row_, my_CrossValScore\n",
    "from sklearn.model_selection import cross_val_score, GroupShuffleSplit\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "%matplotlib inline\n",
    "my_r2_score = make_scorer(r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable setting\n",
    "\n",
    "We load the naming conventions from 'config.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_Dict = dict()\n",
    "with open('config_EcolPtai.txt') as Conf:\n",
    "    myline = Conf.read().splitlines()\n",
    "    for line in myline:\n",
    "        if not line.startswith('#'):\n",
    "            (key, val) = line.split(':', 1)\n",
    "            Name_Dict[str(key.strip())] = val.strip()\n",
    "        \n",
    "\n",
    "Data_File = Name_Dict['Data_File']\n",
    "# extract the filename for naming of newly generated files\n",
    "File_Base = Name_Dict['File_Base']\n",
    "# the generated files will be stored in a subfolder with custom name\n",
    "Data_Folder = Name_Dict['Data_Folder']\n",
    "# column name of expression values\n",
    "Y_Col_Name = eval(Name_Dict['Y_Col_Name'])\n",
    "# figure file type\n",
    "Fig_Type = Name_Dict['Figure_Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "General information on the data source csv-file is stored in the 'config.txt' file generated in the '0-Workflow' notebook. The sequence and expression data is stored in a csv file with an identifier in column 'ID' (not used for anything), the DNA-sequence in column 'Sequence', and the expression strength in column 'promoter activity'. While loading, the sequence is converted to a label encrypted sequence, ['A','C','G','T'] replaced by [0,1,2,3], and a one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeqDat = Data_Src_Load(Name_Dict)\n",
    "SeqDat.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation\n",
    "\n",
    "For the machine learning the data is first separated into training and test sets. The training set is used to generate a standard scaler for expression standardization to zero mean and unit variance. On each position the entropy is calculated to assess how much nucleotide diversity has been sampled on each position. If at any position the entropy is zero, i.e. only one nucleotide is present in all samples, this position is removed because it is non-informative for further analysis (Position entropy analysis). \n",
    "\n",
    "### Loading train and test set used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTest_File = os.path.join(Data_Folder, '{}_{}_TrainTest-Data.pkl'.format(Name_Dict['RFR_Date'], File_Base))\n",
    "TrainTest_Data = pickle.load(open(TrainTest_File,'rb'))\n",
    "SeqTrain, SeqTest = TrainTest_Data['Train'], TrainTest_Data['Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor file identifier for date\n",
    "RFR_Date = Name_Dict['RFR_Date']\n",
    "\n",
    "# Number of independent promoter library measurements\n",
    "Measure_Numb = int(Name_Dict['Library_Expression'])\n",
    "RFR_Best = dict()\n",
    "Feature_Importance = dict()\n",
    "Expr_Scaler = dict()\n",
    "Y_train = np.empty((SeqTrain.shape[0], Measure_Numb))\n",
    "Y_train_pred_ori = np.empty((SeqTrain.shape[0], Measure_Numb))\n",
    "Y_test = np.empty((SeqTest.shape[0], Measure_Numb))\n",
    "Y_test_pred_ori = np.empty((SeqTest.shape[0], Measure_Numb))\n",
    "scores = np.empty((10, Measure_Numb))\n",
    "\n",
    "r2_train = np.empty(Measure_Numb)\n",
    "r2_test = np.empty(Measure_Numb)\n",
    "rmse_train = np.empty(Measure_Numb)\n",
    "rmse_test = np.empty(Measure_Numb)\n",
    "print('Loading regressor')\n",
    "for Meas_Idx in range(Measure_Numb): \n",
    "#     [RFR_Path, RFR_File] = os.path.split(RFR_File)\n",
    "#     [RFR_Date, RFR_ID, RFR_Proc, RFR_Task] = RFR_File.split('_')\n",
    "    ML_Regressor_ID = Name_Dict['RFR_ML_File']\n",
    "    RFR_File = os.path.join(Data_Folder, '{}_{}_{}_{}.pkl'.format(RFR_Date, File_Base, Y_Col_Name[Meas_Idx].replace(' ','-'), ML_Regressor_ID))\n",
    "    ML_Param_ID = Name_Dict['RFR_Params_File']\n",
    "    Parameter_File = os.path.join(Data_Folder, '{}_{}_{}_{}.pkl'.format(RFR_Date, File_Base, Y_Col_Name[Meas_Idx].replace(' ','-'), ML_Param_ID))\n",
    "    \n",
    "    try:\n",
    "        RFR_DictName = '{}_RFR'.format(Y_Col_Name[Meas_Idx])\n",
    "        RFR_Best[RFR_DictName] = joblib.load(RFR_File)\n",
    "        FI_DictName = '{}_FI'.format(Y_Col_Name[Meas_Idx])\n",
    "        Feature_Importance[FI_DictName] = np.array(RFR_Best[RFR_DictName].feature_importances_).reshape(-1,4)\n",
    "        # extracting standard scaler from existing random forest regressor\n",
    "        # I assume the parameters have been generated in the same run as the regressor itself and is located in the same directory following the default naming scheme\n",
    "        RFR_Parameter = pickle.load(open(Parameter_File,'rb'))\n",
    "        # The standard scaler default name is the name of the expression measurement column with suffix: '_Scaler'\n",
    "        Scaler_DictName = '{}_Scaler'.format(Y_Col_Name[Meas_Idx])\n",
    "        Expr_Scaler[Scaler_DictName] = RFR_Parameter[Scaler_DictName]\n",
    "        Positions_removed = RFR_Parameter['Positions_removed']\n",
    "    except FileNotFoundError:\n",
    "        print('Target random forest regressor file not found.')\n",
    "        \n",
    "    X_tmp = list_onehot(list(np.delete(np.array(list(SeqTrain['Sequence_label-encrypted'])),Positions_removed, axis=1)))\n",
    "    X_train = np.array(X_tmp).reshape(len(SeqTrain.index),-1)\n",
    "    # activity prediction of training set with best random forest estimator\n",
    "    Y_train_pred_ori[:,Meas_Idx] = Expr_Scaler[Scaler_DictName].inverse_transform(RFR_Best[RFR_DictName].predict(X_train))\n",
    "\n",
    "    # Test set prediction\n",
    "    # removing sequence positions that were missing in the feature vector for ml\n",
    "    # getting one-hot encodings from the original train and test data\n",
    "    X_tmp = list_onehot(list(np.delete(np.array(list(SeqTest['Sequence_label-encrypted'])),Positions_removed, axis=1)))\n",
    "    X_test = np.array(X_tmp).reshape(len(SeqTest.index),-1)\n",
    "    # activity prediction of training set with best random forest estimator\n",
    "    Y_test_pred_ori[:,Meas_Idx] = Expr_Scaler[Scaler_DictName].inverse_transform(RFR_Best[RFR_DictName].predict(X_test))\n",
    "\n",
    "    # corresponding observations scaled\n",
    "    Scaler_DictName = '{}_Scaler'.format(Y_Col_Name[Meas_Idx])\n",
    "    Y_train[:, Meas_Idx] = SeqTrain[Y_Col_Name[Meas_Idx]].values\n",
    "    Y_test[:, Meas_Idx] = SeqTest[Y_Col_Name[Meas_Idx]].values\n",
    "\n",
    "    r2_train[Meas_Idx] = r2_score(Y_train[:, Meas_Idx], Y_train_pred_ori[:, Meas_Idx])\n",
    "    r2_test[Meas_Idx] = r2_score(Y_test[:, Meas_Idx], Y_test_pred_ori[:, Meas_Idx])\n",
    "    rmse_train[Meas_Idx] = sqrt(mean_squared_error(Y_train[:, Meas_Idx], Y_train_pred_ori[:, Meas_Idx]))\n",
    "    rmse_test[Meas_Idx] = sqrt(mean_squared_error(Y_test[:, Meas_Idx], Y_test_pred_ori[:, Meas_Idx]))\n",
    "    \n",
    "#     # cross-validation scoring\n",
    "    cv = GroupShuffleSplit(n_splits=10, test_size=.1, random_state=42)\n",
    "#     Y_train_scaled = np.ravel(Expr_Scaler[Scaler_DictName].transform(SeqTrain[Y_Col_Name[Meas_Idx]].values.reshape(-1, 1)))\n",
    "    groups = SeqTrain['Sequence_letter-encrypted']\n",
    "#     scores[Meas_Idx] = cross_val_score(RFR_Best[RFR_DictName], X_train, Y_train_scaled, groups=groups, cv=cv)# , groups=groups, scoring=my_r2_score\n",
    "#     scores[:, Meas_Idx] = my_CrossValScore(X_train, Y_train[:, Meas_Idx], groups, cv, RFR_Best[RFR_DictName], my_r2_score)\n",
    "\n",
    "\n",
    "print('training set R2 score: {}, RMSE: {}'.format(np.round(r2_train,2), rmse_train))\n",
    "print('test set R2 score: {}, RMSE: {}'.format(np.round(r2_test,2), rmse_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecv = GroupShuffleSplit(n_splits=1, test_size=.1)\n",
    "mycv = list(onecv.split(X_train, Y_train, groups))\n",
    "onetrain = mycv[0][0]\n",
    "onetest = mycv[0][1]\n",
    "ML_fun = RFR_Best['Ecol Promoter Activity_RFR']\n",
    "ML_fun.fit(X_train[onetrain], Y_train[onetrain,0])\n",
    "print(my_r2_score(ML_fun, X_train[onetrain], Y_train[onetrain,0]))\n",
    "print(my_r2_score(ML_fun, X_train[onetest], Y_train[onetest,0]))\n",
    "plt.scatter(ML_fun.predict(X_train[onetest]),Y_train[onetest,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ExpressionExpert_Functions import my_CrossValScore\n",
    "cv.n_splits = 10\n",
    "myscores = my_CrossValScore(X_train, Y_train[:,0], groups, cv, ML_fun, my_r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Y_train_scaled = np.ravel(Expr_Scaler[Scaler_DictName].transform(SeqTrain[Y_Col_Name[1]].values.reshape(-1, 1)))\n",
    "myscores = list()\n",
    "for idx in range(cv.n_splits):\n",
    "    mytrain = list(cv.split(X_train, Y_train, groups))[idx][0]\n",
    "    mytest = list(cv.split(X_train, Y_train, groups))[idx][1]\n",
    "    X_sub = X_train[mytrain]\n",
    "#     Y_sub_exp = Expr_Scaler[Scaler_DictName].transform(Y_train[mytrain])\n",
    "    RFR_Best[RFR_DictName].fit(X_sub, Y_train[mytrain,1]) #sub_exp[:,1]\n",
    "    print(Y_train[mytrain,1])\n",
    "    X_sub_test = X_train[mytest]\n",
    "#     Y_sub_exp_test = Expr_Scaler[Scaler_DictName].transform(Y_train[mytest])\n",
    "#     Y_sub_pre = RFR_Best[RFR_DictName].predict(X_sub_test)\n",
    "    myscores.append(my_r2_score(RFR_Best[RFR_DictName], X_sub_test, Y_train[mytest,1]))\n",
    "\n",
    "Y_train_scaled = np.ravel(Expr_Scaler[Scaler_DictName].transform(SeqTrain[Y_Col_Name[Meas_Idx]].values.reshape(-1, 1)))\n",
    "Y_all_sca = Expr_Scaler[Scaler_DictName].transform(Y_train)\n",
    "# Y_train_scaled\n",
    "score = cross_val_score(RFR_Best[RFR_DictName], X_train, Y_all_sca, groups, cv=cv, scoring=my_r2_score)\n",
    "print('crossval: ', score.mean())\n",
    "print('manual: ', np.array(myscores).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "ML_pipe = make_pipeline(preprocessing.StandardScaler(), RFR_Best['Ptai Promoter Activity_RFR'])\n",
    "cross_val_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance visualization\n",
    "### Calculation of model predictions\n",
    "\n",
    "Plot of predicted to measured expression strength for training and test data sets and R$^2$ correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of independent promoter library measurements\n",
    "Measure_Numb = int(Name_Dict['Library_Expression'])\n",
    "Expr_Unit = Name_Dict['Expression_Unit']\n",
    "\n",
    "for Meas_Idx in range(Measure_Numb): \n",
    "#     fig, axs = plt.subplots(nrows=1, ncols=1)\n",
    "    plt.scatter(Y_train[:, Meas_Idx], Y_train_pred_ori[:, Meas_Idx], marker='+')\n",
    "    plt.scatter(Y_test[:, Meas_Idx], Y_test_pred_ori[:, Meas_Idx], marker='o', c='r')\n",
    "    plt.title(Y_Col_Name[Meas_Idx])\n",
    "    plt.xlabel('measured {}'.format(Expr_Unit))\n",
    "    plt.ylabel('predicted {}'.format(Expr_Unit))\n",
    "    plt.legend(['Training set, R$^2$={:.2f}'.format(r2_train[Meas_Idx]),'Test set, R$^2$={:.2f}'.format(r2_test[Meas_Idx])], loc='upper left')\n",
    "\n",
    "    # saving the figure\n",
    "    Fig_ID = Name_Dict['CorrPlot_File']\n",
    "    CorrPlot_File = os.path.join(Data_Folder, '{}_{}_{}_{}.{}'.format(time.strftime('%Y%m%d'), File_Base, Fig_ID, Y_Col_Name[Meas_Idx].replace(' ','-'), Fig_Type))\n",
    "    plt.savefig(CorrPlot_File, bbox_inches='tight', format=Fig_Type)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence logo for feature importance\n",
    "\n",
    "The feature importance of the random forest regressor, i.e. the y-axis in the Logo-plot, is normalized to sum over all nucleotide-positions to one.\n",
    "\n",
    "The logos are generated with [Logomaker](https://logomaker.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of independent promoter library measurements\n",
    "Measure_Numb = int(Name_Dict['Library_Expression'])\n",
    "Expr_Unit = Name_Dict['Expression_Unit']\n",
    "\n",
    "for Meas_Idx in range(Measure_Numb): \n",
    "    MeasName = '{}_FI'.format(Y_Col_Name[Meas_Idx])\n",
    "    PWM_tmp = pd.DataFrame(Feature_Importance[MeasName], columns=['A','C','G','T'])\n",
    "    PWM_best = Insert_row_(Positions_removed, PWM_tmp, np.zeros([len(Positions_removed),4]))\n",
    "    nn_logo = lm.Logo(PWM_best)\n",
    "    nn_logo.ax.set_xlabel('Position')\n",
    "    nn_logo.ax.set_ylabel(r'$\\frac{Importance_i}{\\sum Importance}$')\n",
    "    nn_logo.ax.set_title(Y_Col_Name[Meas_Idx])\n",
    "    \n",
    "    # saving the figure\n",
    "    Fig_ID = Name_Dict['LogoPlot_File']\n",
    "    LogoPlot_File = os.path.join(Data_Folder, '{}_{}_{}_{}.{}'.format(time.strftime('%Y%m%d'), File_Base, Fig_ID, Y_Col_Name[Meas_Idx].replace(' ','-'), Fig_Type))\n",
    "#     plt.savefig(LogoPlot_File, bbox_inches='tight', format=Fig_Type)\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
