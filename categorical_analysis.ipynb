{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorization test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import time\n",
    "import timeit\n",
    "import joblib\n",
    "import pickle\n",
    "from ExpressionExpert_Functions import init_Exp2, Data_Src_Load, make_DataDir, split_train_test, ExpressionScaler, Sequence_Conserved_Adjusted, MyRF, MySV, MyGB\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_Dict = init_Exp2('config_Pput.txt')\n",
    "\n",
    "ML_Date = Name_Dict['ML_Date']\n",
    "File_Base = Name_Dict['Data_File'].split('.')[0]\n",
    "Data_Folder = 'data-{}'.format(File_Base) \n",
    "Measure_Numb = int(Name_Dict['Library_Expression'])\n",
    "ML_Regressor = Name_Dict['ML_Regressor'][:-1]\n",
    "ML_Type = Name_Dict['ML_Regressor'][-1]\n",
    "Y_Col_Name = eval(Name_Dict['Y_Col_Name'])\n",
    "Meas_Name = '{}_ML'.format(Y_Col_Name[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeqDat = Data_Src_Load(Name_Dict)\n",
    "SeqDat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('new train-test')\n",
    "train_size = 1 - eval(Name_Dict['TestRatio'])\n",
    "# split number '1' because we only use one final test set. Cross validation comes later\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=train_size)\n",
    "X = SeqDat['Sequence']\n",
    "y = SeqDat[Y_Col_Name]\n",
    "groups = SeqDat['Sequence_letter-encrypted'].str.upper()\n",
    "Train_Idx, Test_Idx = list(gss.split(X, y, groups))[0]\n",
    "SeqTest = SeqDat.iloc[Test_Idx].reset_index(drop=True)\n",
    "SeqTrain = SeqDat.iloc[Train_Idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = .1\n",
    "split_number = 100\n",
    "\n",
    "# removing non-informative positions where no base diversity exists, base one hot encoding\n",
    "SeqTrain_Hadj, Positions_removed, PSEntropy = Sequence_Conserved_Adjusted(SeqTrain, Name_Dict, Entropy_cutoff=float(Name_Dict['Entropy_cutoff']))\n",
    "SeqOH = SeqTrain_Hadj\n",
    "test_ratio = .1\n",
    "split_number = 100\n",
    "Num=100\n",
    "Validation_cutoff=.1\n",
    "AddFeat = eval(Name_Dict['Add_Feat'])\n",
    "\n",
    "Sequence_Samples, Sequence_Positions, Sequence_Bases = np.array(SeqOH['OneHot'].values.tolist()).shape\n",
    "X = np.array(SeqOH['OneHot'].values.tolist()).reshape(Sequence_Samples,Sequence_Positions*Sequence_Bases)\n",
    "X = np.append(X,np.array(SeqOH[AddFeat]), axis=1)\n",
    "Y = SeqOH[Meas_Name].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import GroupShuffleSplit, GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "groups = SeqOH['Sequence_letter-encrypted']\n",
    "Number_Estimators = np.arange(20,40,2)\n",
    "Max_Features = np.arange(10,30,2)\n",
    "min_samples_split = np.arange(3,4,1)\n",
    "max_depth = np.arange(3,5)\n",
    "min_samples_leaf = np.array([2])\n",
    "param_grid = [{'bootstrap':[False], 'n_estimators': Number_Estimators, 'max_features': Max_Features, 'min_samples_split': min_samples_split, 'max_depth':max_depth, 'min_samples_leaf':min_samples_leaf}]\n",
    "# Group shuffle split removes groups with identical sequences from the development set\n",
    "# This is more realistic for parameter estimation\n",
    "cv = GroupShuffleSplit(n_splits=Num, test_size=Validation_cutoff, random_state=42)\n",
    "\n",
    "forest_grid = RandomForestClassifier()\n",
    "\n",
    "grid_forest = GridSearchCV(forest_grid, param_grid, cv=cv, n_jobs=-1)\n",
    "grid_forest.fit(X, Y, groups)\n",
    "ML_Best = grid_forest.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyReg = MyRF(SeqTrain_Hadj, test_ratio, split_number, Meas_Name, eval(Name_Dict['Response_Value']), AddFeat)\n",
    "ML_Best = MyReg.best_estimator_\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Measure_Name = '{}_ML'.format(Y_Col_Name[0])\n",
    "\n",
    "Regressor_File = os.path.join(Data_Folder, '{}_{}_{}_{}-Regressor.pkl'.format(ML_Date, File_Base, Measure_Name.replace(' ','-'), Name_Dict['ML_Regressor']))\n",
    "Parameter_File = os.path.join(Data_Folder, '{}_{}_{}_{}-Params.pkl'.format(ML_Date, File_Base, Measure_Name.replace(' ','-'), Name_Dict['ML_Regressor']))\n",
    "\n",
    "ML_DictName = '{}_Regressor'.format(Y_Col_Name)\n",
    "ML_Best = joblib.load(Regressor_File)\n",
    "# I assume the parameters have been generated in the same run as the regressor itself and is located in the same directory following the default naming scheme\n",
    "Data_Prep_Params = pickle.load(open(Parameter_File,'rb'))\n",
    "# extracting the positions that were removed because of insufficient information content\n",
    "Positions_removed = Data_Prep_Params['Positions_removed']\n",
    "# if the data was standardized we load the corresponding function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score, GroupShuffleSplit\n",
    "from ExpressionExpert_Functions import list_onehot\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, roc_auc_score, f1_score\n",
    "from sklearn.metrics import make_scorer \n",
    "my_score = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "CVsplit = 25\n",
    "X_tmp = list_onehot(list(np.delete(np.array(list(SeqTrain['Sequence_label-encrypted'])),Positions_removed, axis=1)))\n",
    "X_train = np.array(X_tmp).reshape(len(SeqTrain.index),-1)\n",
    "AddFeat = eval(Name_Dict['Add_Feat'])\n",
    "# adding the additional feature, here GC-content\n",
    "X_train = np.append(X_train,SeqTrain[AddFeat].values, axis=1)\n",
    "# activity prediction of training set with best random forest estimator\n",
    "Y_train_pred = ML_Best.predict(X_train)\n",
    "\n",
    "# Test set prediction\n",
    "# removing sequence positions that were missing in the feature vector for ml\n",
    "# getting one-hot encodings from the original train and test data\n",
    "X_tmp = list_onehot(list(np.delete(np.array(list(SeqTest['Sequence_label-encrypted'])),Positions_removed, axis=1)))\n",
    "X_test = np.array(X_tmp).reshape(len(SeqTest.index),-1)\n",
    "# adding the additional feature, here GC-content\n",
    "X_test = np.append(X_test,SeqTest[AddFeat].values, axis=1)\n",
    "# activity prediction of training set with best random forest estimator\n",
    "Y_test_pred = ML_Best.predict(X_test)\n",
    "\n",
    "# corresponding observations scaled\n",
    "#     Scaler_DictName = '{}_Scaler'.format(Y_Col_Name[Meas_Idx])\n",
    "Y_train = SeqTrain[Meas_Name].values\n",
    "Y_test = SeqTest[Meas_Name].values\n",
    "\n",
    "# r2_train[Meas_Idx] = r2_score(Y_train[:, Meas_Idx], Y_train_pred[:, Meas_Idx])\n",
    "# r2_test[Meas_Idx] = r2_score(Y_test[:, Meas_Idx], Y_test_pred[:, Meas_Idx])\n",
    "# rmse_train[Meas_Idx] = sqrt(mean_squared_error(Y_train[:, Meas_Idx], Y_train_pred[:, Meas_Idx]))\n",
    "# rmse_test[Meas_Idx] = sqrt(mean_squared_error(Y_test[:, Meas_Idx], Y_test_pred[:, Meas_Idx]))\n",
    "\n",
    "# cross-validation scoring\n",
    "cv = GroupShuffleSplit(n_splits=CVsplit, test_size=.1, random_state=42)\n",
    "# if applicable correcting target variable according to the standardization\n",
    "# Y_train2 = Y_train\n",
    "groups = SeqTrain['Sequence_letter-encrypted']\n",
    "scores = cross_val_score(ML_Best, X_train, Y_train, groups=groups, scoring=my_score, cv=cv)# , groups=groups, scoring=my_r2_score\n",
    "\n",
    "print('Cross validation for ML-Type: {}'.format(ML_Type))\n",
    "print('R2 Statistic: {:.2f} (+/-{:.2f})'.format(scores.mean(), scores.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "classes = ['low','medium','high']\n",
    "plot_confusion_matrix(ML_Best, X_test, Y_test, display_labels=classes)\n",
    "plot_confusion_matrix(ML_Best, X_train, Y_train, display_labels=classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Balanced accuracy: {}'.format(balanced_accuracy_score(Y_test, Y_test_pred)))\n",
    "print('Weighted f1 score: {:.2f}'.format(f1_score(Y_test, Y_test_pred, average='weighted')))\n",
    "print(classification_report(Y_test, Y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
